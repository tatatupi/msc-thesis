\chapter{Demonstrações}
\label{B}
\section{Demonstração - Equações Matriciais Entrada-Saída}\label{eqmat_in_out}
Num primeiro momento será provada a equação (\ref{y_past_det}).
Portanto, considere o vetor de saídas que varia de $k$ até $k+i-1$:

\begin{equation}
\textbf{y}_k=\left[y_k~y_{k+1}~\cdots~y_{k+i-1}\right]^{T},
\end{equation}
em que $i>n$. Assim, de (\ref{mat_y}), tem-se que:
\begin{eqnarray}
y_{k+1}&=Cx_{k+1} + Du_{k+1}=C(Ax_{k} + Bu_{k}) +
Du_{k+1}\nonumber\\
&= CAx_{k} + \left[\begin{array}{cc}
                       CB & D
                     \end{array}
\right] \left[\begin{array}{c}
                u_{k} \\
                u_{k+1}
              \end{array}\right],~~~~~~~~~~~~~~~~\label{y_k_1_teo1}
\end{eqnarray}
e,
\begin{eqnarray}
x_{k+2}&=Ax_{k+1} + Bu_{k+1} = A(Ax_{k} + Bu_{k}) +
Bu_{k+1}= A^{2}x_{k}+ ABu_{k}+ Bu_{k+1},\\
y_{k+2}&=Cx_{k+2} + Du_{k+2}=C(A^{2}x_{k}+ ABu_{k}+
Bu_{k+1})+ Du_{k+2}\nonumber~~~~~~~~~~~~~~~~~~~~~~\\
&= CA^{2}x_{k} + \left[\begin{array}{ccc}
                                             CAB & CB & D
                                           \end{array}\right]\left[ \begin{array}{c}
                        u_{k} \\
                        u_{k+1} \\
                        u_{k+2}
                      \end{array}
 \right].~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\label{y_k_2_teo1}
\end{eqnarray}

Seguindo o mesmo procedimento até $k+i-1$, obtém-se:

\begin{equation}\label{y_k_i_1_teo1}
y_{k+i-1}=CA^{i-1}x_{k}+ \left[\begin{array}{cccccc}
                                       CA^{i-2}B & CA^{i-3}B & CA^{i-4}B& \cdots & CB &
                                       D
                                     \end{array}
\right]\left[\begin{array}{c}
                u_{k} \\
                u_{k+1} \\
                u_{k+2} \\
               \vdots \\
                u_{k+i-2} \\
                u_{k+i-1}
             \end{array}
 \right].
\end{equation}

Colocando as equações (\ref{mat_y}), (\ref{y_k_1_teo1}),
(\ref{y_k_2_teo1}) e (\ref{y_k_i_1_teo1}) na forma matricial,
tem-se:
\begin{eqnarray}
\left[\begin{array}{c}
        y_{k} \\
        y_{k+1} \\
        y_{k+2} \\
        \vdots \\
        y_{k+i-1}
      \end{array}
\right]_{li \times 1}&=&\left[\begin{array}{c}
        C \\
        CA \\
       CA^{2} \\
       \vdots\\
       CA^{i-1}
      \end{array}
\right]_{li \times n}\left[x_{k}\right]_{n \times 1}+\nonumber\\
&+&\left[\begin{array}{ccccc}
           D & 0 & 0 & 0 \cdots & 0 \\
           CB & D & 0 & \cdots & 0 \\
           CAB & CB & D & \ddots & 0 \\
           \vdots & \vdots & \vdots & \ddots & \vdots \\
           CA^{i-2}B & CA^{i-3}B & CA^{i-4}B  & \cdots & D
         \end{array}
\right]_{li \times mi}\left[\begin{array}{c}
                              u_{k} \\
                              u_{k+1} \\
                              u_{k+2} \\
                              \vdots \\
                              u_{k+i-1}
                            \end{array}
\right]_{mi \times 1}.
\end{eqnarray}

Empregando-se notação de matriz de observabilidade estendida
(\ref{observabilidade_estendida_1}) e de matriz em blocos triangular
inferior de Toeplitz (\ref{toeplitz_det}), obtém-se:

\begin{equation}
\left[\begin{array}{c}
        y_{k} \\
        y_{k+1} \\
        y_{k+2} \\
        \vdots \\
        y_{k+i-1}
      \end{array}
\right]=\Gamma_{i}x_{k}+H^{d}_{i}\left[\begin{array}{c}
                              u_{k} \\
                              u_{k+1} \\
                              u_{k+2} \\
                              \vdots \\
                              u_{k+i-1}
                            \end{array}
\right].
\end{equation}

Em seguida, varia-se $k$ de $0$ a $j-1$, como a seguir:
\begin{flushleft}
\begin{minipage}{10.75cm}
\begin{eqnarray}
&\text{\normalsize{Para $k=0$,~então}}&\left[\begin{array}{c}
        y_{0} \\
        y_{1} \\
        y_{2} \\
        \vdots \\
        y_{i-1}
      \end{array}
\right]=\Gamma_{i}x_{0}+H^{d}_{i}\left[\begin{array}{c}
                              u_{0} \\
                              u_{1} \\
                              u_{2} \\
                              \vdots \\
                              u_{i-1}
                            \end{array}
\right],\nonumber\\
&\text{\normalsize{Para $k=1$,~então}}&\left[\begin{array}{c}
        y_{1} \\
        y_{2} \\
        y_{3} \\
        \vdots \\
        y_{i}
      \end{array}
\right]=\Gamma_{i}x_{1}+H^{d}_{i}\left[\begin{array}{c}
                              u_{1} \\
                              u_{2} \\
                              u_{3} \\
                              \vdots \\
                              u_{i}
                            \end{array}
\right],\nonumber\\
&\text{\normalsize{Para $k=j-1$,~então}}&\left[\begin{array}{c}
        y_{j-1} \\
        y_{j} \\
        y_{j+1} \\
        \vdots \\
        y_{i+j-2}
      \end{array}
\right]=\Gamma_{i}x_{j-1}+H^{d}_{i}\left[\begin{array}{c}
                              u_{j-1} \\
                              u_{j} \\
                              u_{j+1} \\
                              \vdots \\
                              u_{i+j-2}
                            \end{array}
\right].\nonumber
\end{eqnarray}
\end{minipage}
\end{flushleft}

Expressando essas equações em matrizes de blocos, obtém-se:

\begin{eqnarray}
\left[\begin{array}{cccc}
        y_{0} & y_{1} & \cdots & y_{j-1} \\
        y_{1} & y_{2} & \cdots & y_{j} \\
        y_{2} & y_{3} & \cdots & y_{j+1} \\
        \vdots & \vdots & \cdots & \vdots \\
        y_{i-1} & y_{i} & \cdots & y_{i+j-2}
      \end{array}
\right]&=&\Gamma_{i}\left[\begin{array}{ccccc}
                            x_{0} & x_{1} & \cdots & x_{j-2} & x_{j-1}
                          \end{array}
\right]+\nonumber\\
&+&H^{d}_{i}\left[\begin{array}{cccc}
        u_{0} & u_{1} & \cdots & u_{j-1} \\
        u_{1} & u_{2} & \cdots & u_{j} \\
        u_{2} & u_{3} & \cdots & u_{j+1} \\
        \vdots & \vdots & \cdots & \vdots \\
        u_{i-1} & u_{i} & \cdots & u_{i+j-2}
      \end{array}\right].
\end{eqnarray}

Portanto, pode-se concluir que:
\begin{equation}
   Y_{p}=\Gamma_{i}X_{p} + H^{d}_{i}U_{p}.\nonumber
\end{equation}

Agora será provada a equação (\ref{y_future_det}). Considere o
seguinte vetor de saídas que varia de $k+i$ até $k+2i-1$:

\begin{equation}
\textbf{y}_{k+i}=\left[y_{k+i}~y_{k+i+1}~\cdots~y_{k+2i-1}\right]^{T},
\end{equation}
em que $i>n$. Seguindo o mesmo raciocínio utilizado para provar
(\ref{y_past_det}), fazendo-se substituições sucessivas para cada
medida do vetor $\textbf{y}_{k+i}$, obtém-se:

\begin{eqnarray}
\left[\begin{array}{c}
        y_{k+i} \\
        y_{k+i+1} \\
        y_{k+i+2} \\
        \vdots \\
        y_{k+2i-1}
      \end{array}
\right]_{li \times 1}&=&\left[\begin{array}{c}
        C \\
        CA \\
       CA^{2} \\
       \vdots\\
       CA^{i-1}
      \end{array}
\right]_{li \times n}\left[x_{k+i}\right]_{n \times 1}+\nonumber\\
&+&\left[\begin{array}{ccccc}
           D & 0 & 0 & 0 \cdots & 0 \\
           CB & D & 0 & \cdots & 0 \\
           CAB & CB & D & \ddots & 0 \\
           \vdots & \vdots & \vdots & \ddots & \vdots \\
           CA^{i-2}B & CA^{i-3}B & CA^{i-4}B  & \cdots & D
         \end{array}
\right]_{li \times mi}\left[\begin{array}{c}
                              u_{k+i} \\
                              u_{k+i+1} \\
                              u_{k+i+2} \\
                              \vdots \\
                              u_{k+2i-1}
                            \end{array}
\right]_{mi \times 1}.
\end{eqnarray}

Variando-se $k$ de $0$ a $j-1$ e expressando as equações resultantes
na forma de matrizes de blocos, resulta em:

\begin{eqnarray}
\left[\begin{array}{cccc}
        y_{i} & y_{i+1} & \cdots & y_{i+j-1} \\
        y_{i+1} & y_{i+2} & \cdots & y_{i+j} \\
        y_{i+2} & y_{i+3} & \cdots & y_{i+j+1} \\
        \vdots & \vdots & \cdots & \vdots \\
        y_{2i-1} & y_{2i} & \cdots & y_{2i+j-2}
      \end{array}
\right]&=&\Gamma_{i}\left[\begin{array}{ccccc}
                            x_{i} & x_{i+1} & \cdots & x_{i+j-2} & x_{i+j-1}
                          \end{array}
\right]+\nonumber\\
&+&H^{d}_{i}\left[\begin{array}{cccc}
        u_{i} & u_{i+1} & \cdots & u_{i+j-1} \\
        u_{i+1} & u_{i+2} & \cdots & u_{i+j} \\
        u_{i+2} & u_{i+3} & \cdots & u_{i+j+1} \\
        \vdots & \vdots & \cdots & \vdots \\
        u_{2i-1} & u_{2i} & \cdots & u_{2i+j-2}
      \end{array}\right].
\end{eqnarray}

Dessa forma, conclui-se que:

\begin{equation}
   Y_{f}=\Gamma_{i}X_{f} + H^{d}_{i}U_{f}. \nonumber
\end{equation}

Por fim, será provada a equação (\ref{x_future_det}). Expandindo-se
a equação (\ref{mat_x}), obtém-se:

\begin{eqnarray}
x_{k+1}&=&Ax_{k} + Bu_{k},\\
x_{k+2}&=& A^{2}x_{k}+ ABu_{k}+ Bu_{k+1},\\
x_{k+3}&=& A^{3}x_{k}+A^{2}Bu_{k}+  ABu_{k+1}+ Bu_{k+2},\\
\cdots&~~~~~~~~~~&\cdots~~~~~~~~~~\cdots~~~~~~~~~~\cdots~~~~~~~~~~\cdots\nonumber\\
x_{k+i-1}&=& A^{i-1}x_{k}+A^{i-2}Bu_{k}+
A^{i-3}Bu_{k+1}+\ldots ABu_{k+i-3}+Bu_{k+i-2},\\
x_{k+i}&=& A^{i}x_{k}+A^{i-1}Bu_{k}+ A^{i-2}Bu_{k+1}+\ldots
ABu_{k+i-2}+Bu_{k+i-1}.\label{ext_dedu}
\end{eqnarray}

A equação (\ref{ext_dedu}) pode ser expressa como a seguir:
\begin{equation}
x_{k+i}=\left[\begin{array}{c}
                    A^{i}
                  \end{array}
\right]_{n\times n}\left[\begin{array}{c}
                    x_{k}
                  \end{array}\right]_{n\times 1} +
\left[\begin{array}{c|c|c|c|c}
             A^{i-1}B & A^{i-2}B & \cdots & AB & B
           \end{array}
\right]_{n \times mi}\left[\begin{array}{c}
                             u_{k} \\
                             u_{k+1} \\
                             \vdots  \\
                              u_{k+i-2}\\
                              u_{k+i-1}
                           \end{array}
\right]_{mi \times 1}.
\end{equation}

Empregando-se notação de matriz de controlabilidade estendida
(\ref{controlabilidade_estendida_det}), obtém-se:
\begin{equation}
x_{k+i}=A^{i}x_{k}+\Delta^{d}_{i}\left[\begin{array}{c}
                             u_{k} \\
                             u_{k+1} \\
                             \vdots  \\
                              u_{k+i-2}\\
                              u_{k+i-1}
                           \end{array}
\right].
\end{equation}

Variando-se $k$ de $0$ a $j-1$, obtém-se a seguinte equação na forma
de matrizes de blocos:

\begin{eqnarray}
\left[\begin{array}{ccccc}\label{x_future_det_demo}
                            x_{i} & x_{i+1} & \cdots & x_{i+j-2} & x_{i+j-1}
                          \end{array}
\right]&=&A^{i}\left[\begin{array}{ccccc}
                            x_{0} & x_{1} & \cdots & x_{j-2} & x_{j-1}
                          \end{array}
\right]+\nonumber\\
&+&\Delta^{d}_{i}\left[\begin{array}{cccc}
        u_{0} & u_{1} & \cdots & u_{j-1} \\
        u_{1} & u_{2} & \cdots & u_{j} \\
        u_{2} & u_{3} & \cdots & u_{j+1} \\
        \vdots & \vdots & \cdots & \vdots \\
        u_{i-1} & u_{i} & \cdots & u_{i+j-2}
      \end{array}\right].
\end{eqnarray}

Portanto a equação (\ref{x_future_det_demo}), pode ser expressa como
(\ref{x_future_det}):
\begin{equation}
   X_{f}=A^{i}X_{p} + \Delta^{d}_{i}U_{p}.\nonumber
\end{equation}
\begin{flushright}
$\Box$
\end{flushright}
\section{Demonstração - Teorema \ref{teo_2}}\label{demons_teo_2}

Deseja-se escrever os estados futuros $X_{f}$ como uma combinação
linear dos dados de entradas passadas e futuras. Isto pode ser
obtido relacionando as equações (\ref{y_past_det}) e
(\ref{x_future_det}). Para este desenvolvimento é necessário
explicitar os $X_{p}$ na equação (\ref{x_future_det}) para, então,
substituí-los na equação (\ref{y_past_det}). Contudo, a matriz
$\Gamma_{i} \in \mathbb{R}^{li \times n}$ não é quadrada,
portanto não pode ser invertida. Sendo assim, %será utilizado um
%artifício amplamente conhecido em identificação de sistemas
%(\cite{Aguir2007}). Em um primeiro momento,
pré-multiplicando a equação (\ref{y_past_det}) por $\Gamma^{T}_{i}$
de ambos os lados, resulta em

\begin{equation}\label{y_p_multi_gamma_t}
\Gamma^{T}_{i}Y_{p} =
\Gamma^{T}_{i}\Gamma_{i}X_{p}+\Gamma^{T}_{i}H^{d}_{i}U_{p}.
\end{equation}

Reorganizando (\ref{y_p_multi_gamma_t}), obtém-se
\begin{equation}
\Gamma^{T}_{i}\Gamma_{i}X_{p}=\Gamma^{T}_{i}Y_{p}
-\Gamma^{T}_{i}H^{d}_{i}U_{p}.
\end{equation}

Tem-se que o produto de uma matriz pela sua transposta é uma matriz
quadrada, supondo-se que $\left[\Gamma^{T}_{i}\Gamma_{i}\right]$
seja não singular, obtém-se a seguinte expressão

\begin{equation}\label{x_p_proof_det}
X_{p}=\left[\Gamma^{T}_{i}\Gamma_{i}\right]^{-1}\Gamma^{T}_{i}Y_{p}
-\left[\Gamma^{T}_{i}\Gamma_{i}\right]^{-1}\Gamma^{T}_{i}H^{d}_{i}U_{p}.
\end{equation}

A matriz $\left[\Gamma^{T}_{i}\Gamma_{i}\right]^{-1}\Gamma^{T}_{i}$
é conhecida como matriz pseudo-inversa de Moore-Penrose de
$\Gamma_{i}$ e é denotada por
$\Gamma^{\dag}_{i}\triangleq\left[\Gamma^{T}_{i}\Gamma_{i}\right]^{-1}\Gamma^{T}_{i}$.
Então a equação (\ref{x_p_proof_det}) pode ser reescrita como
\begin{equation}\label{x_d_p_mod}
X_{p}=\Gamma^{\dag}_{i}Y_{p} -\Gamma^{\dag}_{i}H^{d}_{i}U_{p}.
\end{equation}

Logo, substituindo-se (\ref{x_d_p_mod}) em (\ref{x_future_det}),
obtém-se:
\begin{eqnarray}
X_{f}&=&A^{i}(\Gamma^{\dag}_{i}Y_{p}
-\Gamma^{\dag}_{i}H^{d}_{i}U_{p})
+ \Delta^{d}_{i}U_{p}, \nonumber\\
&=&(\Delta^{d}_{i}-A^{i}\Gamma^{\dag}_{i}H^{d}_{i})U_{p}+(A^{i}\Gamma^{\dag}_{i})Y_{p}.\label{x_f_d_mod}
\end{eqnarray}

Por fim, colocando (\ref{x_f_d_mod}) na forma de matriz, tem-se que:

\begin{equation}\label{xdf_analog}
X_{f}=\left[\begin{array}{cc}
                  \Delta^{d}_{i}-A^{i}\Gamma^{\dag}_{i}H^{d}_{i} & A^{i}\Gamma^{\dag}_{i}
                \end{array}
\right]\left[\begin{array}{c}
                U_{p} \\
                Y_{p}
              \end{array}
\right].
\end{equation}

Como mostrado na equação (\ref{xdf_analog}) os estados futuros estão
contidos nos dados de entradas e saídas passados. Agora, será
mostrado que estes estados não estão somente contidos nos dados de
entradas e saídas passadas, mas também nos dados de entradas e
saídas futuras. Essa demonstração, naturalmente, mostra como os
estados são obtidos por meio dos dados medidos. De
(\ref{xdf_analog}), pode-se fazer a seguinte analogia

\begin{equation}\label{x_f_d_comb}
X_{f}=L_{p}W_{p},
\end{equation}
com
\begin{equation}
L_{p}=\left[\begin{array}{cc}
                  \Delta^{d}_{i}-A^{i}\Gamma^{\dag}_{i}H^{d}_{i} & A^{i}\Gamma^{\dag}_{i}
                \end{array}
\right].
\end{equation}

Sendo assim, a equação (\ref{y_future_det}) pode-ser reescrita como:

\begin{equation}\label{yf_demo}
   Y_{f}=\Gamma_{i}L_{p}W_{p} + H^{d}_{i}U_{f}.
\end{equation}

Aplicando a projeção ortogonal sobre o complemento ortogonal do
espaço linha da matriz $U_{f}$ em $Y_{f}$, tem-se:

\begin{equation}\label{yf_proj_uf_com}
   Y_{f}\Pi_{U^{\perp}_{f}}=\Gamma_{i}L_{p}W_{p}\Pi_{U^{\perp}_{f}} + H^{d}_{i}U_{f}\Pi_{U^{\perp}_{f}}.
\end{equation}

Deve ser notado que
$U_{f}\Pi_{U^{\perp}_{f}}=U_{f}(I-\Pi_{U_{f}})=U_{f}-U_{f}\Pi_{U_{f}}=U_{f}-U_{f}=0$.
Portanto, a equação (\ref{yf_proj_uf_com}) torna-se

\begin{eqnarray}
   Y_{f}\Pi_{U^{\perp}_{f}}&=&\Gamma_{i}L_{p}W_{p}\Pi_{U^{\perp}_{f}},\nonumber\\
   Y_{f}/{U^{\perp}_{f}}&=&\Gamma_{i}L_{p}W_{p}/{U^{\perp}_{f}}.\label{proj_demons_teorema}
\end{eqnarray}

Pós-multiplicando-se ambos os lados de (\ref{proj_demons_teorema})
por $\left[W_{p}/{U^{\perp}_{f}}\right]^{\dag}W_{p}$, obtém-se:

\begin{eqnarray}
\left[Y_{f}/U^{\perp}_{f}\right]\left[W_{p}/U^{\perp}_{f}\right]^{\dag}W_{p}&=&\Gamma_{i}L_{p}\left[W_{p}/U^{\perp}_{f}\right]\left[W_{p}/U^{\perp}_{f}\right]^{\dag}W_{p}.\nonumber\\
&=&\Gamma_{i}L_{p}W_{p}.\label{obl_anterior}
\end{eqnarray}
%
%Como demonstrado por \citet[pp. 201-202]{Overschee1996}, partindo-se
%das duas primeiras condições do teorema \ref{teo_2} é possível
%provar que
%$\left[W_{p}/U^{\perp}_{f}\right]\left[W_{p}/U^{\perp}_{f}\right]^{\dag}W_{p}=
%W_{p}$, portanto:
%\begin{equation}\label{obl_anterior}
%\left[Y_{f}/U^{\perp}_{f}\right]\left[W_{p}/U^{\perp}_{f}\right]^{\dag}W_{p}=\Gamma_{i}L_{p}W_{p}.
%\end{equation}

Substituindo-se (\ref{x_f_d_comb}) em (\ref{obl_anterior}),
obtém-se:

%\begin{equation}
%\left[Y_{f}/U^{\perp}_{f}\right]\left[W_{p}/U^{\perp}_{f}\right]^{\dag}W_{p}=\Gamma_{i}X_{f},
%\end{equation}
\begin{equation}\label{obl_anterior_ant}
\left[Y_{f}/U^{\perp}_{f}\right]\left[W_{p}/U^{\perp}_{f}\right]^{\dag}W_{p}=\Gamma_{i}X_{f}.
\end{equation}

Comparando a equação (\ref{obl_anterior_ant}) com a equação
(\ref{proj_obl_A_B_C}), verifica-se que a mesma é uma projeção
oblíqua, portanto:

\begin{equation}\label{obl_det}
\left[Y_{f}/U^{\perp}_{f}\right]\left[W_{p}/U^{\perp}_{f}\right]^{\dag}W_{p}=Y_{f}/_{U_{f}}W_{p}.
\end{equation}

Relacionando a equação definida em (\ref{Oi_def}) com
(\ref{obl_det}), verifica-se que a equação (\ref{obl_anterior_ant})
pode ser escrita como:

\begin{equation}
O_{i}=\Gamma_{i}X_{f}.\nonumber
\end{equation}

Dessa forma a primeira assertiva do Teorema \ref{teo_2} foi provada.
Para provar a segunda assertiva, pré-multiplica-se
$O_{i}=\Gamma_{i}X_{f}$ por $W_{1}$ e pós-multiplica-se por $W_{2}$,
obtendo-se:

\begin{equation}\label{obl_pond_dimen}
[W_{1}]_{li \times li}[O_{i}]_{li \times j}[W_{2}]_{j \times
j}=\underbrace{[W_{1}]_{li \times li}[\Gamma_{i}]_{li \times n}}_{li
\times n}\underbrace{[X_{f}]_{n \times j}[W_{2}]_{j \times j}}_{n
\times j}.
\end{equation}

Como pode ser observado na equação (\ref{obl_pond_dimen}) a matriz
$W_{1}O_{i}W_{2}$ é igual ao produto de duas matrizes,
$W_{1}\Gamma_{i}$ ($n$ colunas) e $X_{f}W_{2}$ ($n$ linhas). Pela
hipótese 3 do Teorema \ref{teo_2} pode se dizer que ambas as
matrizes possuem posto $n$, por conseguinte o produto delas também
terá posto $n$. Portanto, tem-se que a ordem do sistema é $n$ e se
prova a segunda assertiva do teorema. Por meio desta afirmação e de
(\ref{svd_obl_pond_simp}) pode-se reescrever a equação
(\ref{obl_pond_dimen}) como a seguir:

\begin{equation}\label{obl_pond_dimen_USV}
\underbrace{[W_{1}]_{li \times li}[\Gamma_{i}]_{li \times n}}_{li
\times n}\underbrace{[X_{f}]_{n \times j}[W_{2}]_{j \times j}}_{n
\times j}=\underbrace{[U_{1}]_{li \times n}[S_{1}^{1/2}]_{n \times
n}}_{li \times n}\underbrace{[S_{1}^{1/2}]_{n \times
n}[V_{1}]^{T}_{n \times j}}_{n \times j}.
\end{equation}

A equação (\ref{obl_pond_dimen_USV}) pode ser dividida em duas
partes (em que $T \in \mathbb{R}^{n \times n}$ é uma matriz
arbitrária não-singular representando uma transformação de
similaridade):

\begin{equation}\label{w_gamma_igual_us_12T}
[W_{1}]_{li \times li}[\Gamma_{i}]_{li \times n}=[U_{1}]_{li \times
n}[S_{1}^{1/2}]_{n \times n}[T]_{n \times n}~\text{\normalsize{e}},
\end{equation}

\begin{equation}\label{w_gamma_igual_us_12T}
[X_{f}]_{n \times j}[W_{2}]_{j \times j}=[T^{-1}]_{n \times
n}[S_{1}^{1/2}]_{n \times n}[V^{T}_{1}]_{n \times j},
\end{equation}
que leva a terceira e quarta assertiva do Teorema \ref{teo_2}.
Agora, pré-multiplicando-se a equação (\ref{O_i_igual_GammaXdf}) de
ambos os lados pela pseudo-inversa de $\Gamma_{i}$, obtém-se a prova
da quinta assertiva:

\begin{equation}
 X_{f}=\Gamma^{\dag}_{i}O_{i}.\nonumber
\end{equation}
\begin{flushright}
$\Box$
\end{flushright}

\clearpage
